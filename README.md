# Bert-Bertimbal
Performing fine-tuning on BERT and BERTimbau models for text classification in Portuguese using the News Dataset in Portuguese.

-  BERT:
Devlin, J. et al (2018): “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. Available at: https://arxiv.org/abs/1810.04805
Model: https://github.com/google-research/bert


- BERTimbau:
Souza, F. et al (2020): “BERTimbau: Pretrained BERT Models for Brazilian Portuguese”. Available at: https://link.springer.com/chapter/10.1007/978-3-030-61377-8_28
Models: https://github.com/neuralmind-ai/portuguese-bert / https://huggingface.co/neuralmind/bert-base-portuguese-cased


- News Dataset in Portuguese: https://www.kaggle.com/datasets/marlesson/news-of-the-site-folhauol/data
